{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565dd12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from tqdm import tqdm\n",
    "from math import log2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "from scipy.spatial.distance import chebyshev\n",
    "from scipy.spatial.distance import kulsinski\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import sqeuclidean\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "from skmultiflow.drift_detection import HDDM_W\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "import warnings\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_train = 30000\n",
    "length_of_test = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11489171",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train(dataset, length_of_train):\n",
    "    X_train = df.iloc[0:length_of_train]\n",
    "    X_train = X_train.drop(['class'], axis = 1)\n",
    "    y_train = df['class'][0:length_of_train]\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test(dataset, start_from_sample, length_of_test):\n",
    "    X_test = df.iloc[start_from_sample:start_from_sample+length_of_test]\n",
    "    X_test = X_test.drop(['class'], axis = 1)\n",
    "    y_test = df['class'][start_from_sample:start_from_sample+length_of_test]\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbfa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return sum(p[0][i] * log2(p[0][i]/q[0][i]) for i in range(len(p[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhattacharyya(p, q):\n",
    "    return -np.log(sum(np.square(p[0][i]*1.0*q[0][i]) for i in range(len(p[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef03281",
   "metadata": {},
   "source": [
    "Bootstrapping should have as input:\n",
    "- training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66545408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping(X_train, bootstrapping_samples, pca=None):\n",
    "    # Extract distributions of 50 random training subsets\n",
    "    distributions_bootstrapping = []\n",
    "    for i in tqdm(range(0, bootstrapping_samples)):\n",
    "        # generate random number between 0 and length_of_train-length_of_test in order to have a sample of size length_of_test\n",
    "        rand = random.randint(0,length_of_train-length_of_test)\n",
    "        # extract the distribution from the training samples random number:random number + length_of_test by means of histogram\n",
    "        if pca is not None:\n",
    "            bootstrapping_input = pca.transform(X_train[rand:rand+length_of_test])\n",
    "        else:\n",
    "            bootstrapping_input = X_train[rand:rand+length_of_test]\n",
    "        dist = sns.distplot(bootstrapping_input).get_lines()[0].get_data()[1]\n",
    "        # store distribution\n",
    "        distributions_bootstrapping.append(dist)\n",
    "        plt.close()\n",
    "    return distributions_bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9026fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kdqtrees_bootstrapping(distrib_X_train, distrib_X_test, distributions_bootstrapping, distance):\n",
    "    \n",
    "    # option to choose the similarity distance\n",
    "    \n",
    "    if(distance == 'kl_divergence'):\n",
    "        similarity_metric = kl_divergence\n",
    "    elif(distance == 'manhattan'):\n",
    "        similarity_metric = manhattan_distances\n",
    "    elif(distance == 'chebyshev'):\n",
    "        similarity_metric = chebyshev\n",
    "    elif(distance == 'kulsinski'):\n",
    "        similarity_metric = kulsinski\n",
    "    elif(distance == 'cosine'):\n",
    "        similarity_metric = cosine\n",
    "    elif(distance == 'squared_euclidean'):\n",
    "        similarity_metric = sqeuclidean\n",
    "    elif(distance == 'bhattacharyya'):\n",
    "        similarity_metric = bhattacharyya\n",
    "    \n",
    "    # Bootstrapping technique to define critical region\n",
    "    \n",
    "    dist_bootstrapping = []\n",
    "    \n",
    "    # Calculate similarity distance between training set and each of the 50 training subsets\n",
    "    for i in range(0, len(distributions_bootstrapping)):\n",
    "        dist_bootstrapping.append(similarity_metric([distrib_X_train], [distributions_bootstrapping[i]]))\n",
    "    \n",
    "    \n",
    "    # Define Critical Region\n",
    "    critical_region = np.max(dist_bootstrapping)\n",
    "    \n",
    "    # Detect Drifts between train and test\n",
    "    \n",
    "    drift_batch = None\n",
    "    \n",
    "    # Calculate distance between train and test distributions\n",
    "    \n",
    "    similarity_metric_train_test = similarity_metric([distrib_X_train], [distrib_X_test])\n",
    "    \n",
    "    if(similarity_metric_train_test>critical_region):\n",
    "        print(distance + \" : Drift Detected\" )\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6075e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ede_drift_detector(distrib_X_train, distrib_X_test, statistical_test):\n",
    "\n",
    "    if(statistical_test == 'ks'):\n",
    "        stat_test = stats.kstest\n",
    "    elif(statistical_test == 'mw'):\n",
    "        stat_test = stats.mannwhitneyu\n",
    "    \n",
    "    v, p = stat_test(distrib_X_train, distrib_X_test)\n",
    "        \n",
    "    \n",
    "    if(p<=0.05):\n",
    "        print(\"Reject Null Hypothesis according to KS statistical test\")\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_times_detection(array):\n",
    "    results = []\n",
    "\n",
    "    \n",
    "    if(len(array)==0):\n",
    "        results = 'nothing_detected'\n",
    "    else:\n",
    "        for i in range(0, len(array)):\n",
    "            results.append(int(array[i]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc74ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_false_positive(array_batches):\n",
    "    \n",
    "    drift_start = 2\n",
    "    \n",
    "    if(len(array_batches)>0): \n",
    "        \n",
    "        if(len([x for x in array_batches if x<drift_start])>0):\n",
    "            return(len([x for x in array_batches if x<drift_start])/drift_start)\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    else:\n",
    "        return 'nothing_detected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_latency(array_batches):\n",
    "    no_batches_with_drift = 5\n",
    "    drift_start = 2\n",
    "    \n",
    "    if(len(array_batches)>0):  \n",
    "        #print(np.array(array_pr)>=drift_type_start)\n",
    "        #print(np.argwhere(np.array(array_pr)>=drift_type_start).size==0)\n",
    "        \n",
    "        if(np.argwhere(np.array(array_batches)>=drift_start).size==0):\n",
    "            latency_score = 'nothing_detected' \n",
    "        else:\n",
    "            batch_drift_detected = array_batches[np.argwhere(np.array(array_batches)>=drift_start)[0][0]]\n",
    "            latency_score = (batch_drift_detected - drift_start)/no_batches_with_drift\n",
    "        return latency_score\n",
    "    else:\n",
    "        return 'nothing_detected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b185e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fde34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abef664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33091a97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = ['sea', 'agraw1', 'agraw2']\n",
    "drift_types = ['abrupt', 'gradual']\n",
    "random_seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "driftwidth_values = ['05', '1', '5', '10', '20']\n",
    "noise_values = [0, 10, 20]\n",
    "balance_types = ['imbalanced', 'balanced']\n",
    "\n",
    "\n",
    "final_results_df = pd.DataFrame(columns=['dataset', 'drift_type', 'drift_width', 'detector', 'distance/test', \n",
    "                                         'drift_batches','noise_value', 'balance_type', 'false_positive_rate', \n",
    "                                         'latency', 'random_seed'])\n",
    "\n",
    "\n",
    "for data_name in datasets:\n",
    "    for noise_value in noise_values:\n",
    "        for balance_type in balance_types:\n",
    "            for random_seed in random_seeds:\n",
    "                for drift_type in drift_types:\n",
    "                        for driftwidth_value in driftwidth_values:\n",
    "\n",
    "                            # to only go through abrupt once\n",
    "                            if drift_type == 'abrupt' and driftwidth_values.index(driftwidth_value) > 0:\n",
    "                                break\n",
    "\n",
    "                            path = '../../../../Documents/phd_related/data_sets_concept_drift/moa_datasets/'\n",
    "                            dataset_name = data_name\n",
    "\n",
    "\n",
    "                            drift_width = '' if drift_type != 'gradual' else f'_{driftwidth_value}'\n",
    "                            drift_width_column = drift_width if drift_type == 'abrupt' else driftwidth_value\n",
    "\n",
    "                            dataset_path = path + f'{dataset_name}_{random_seed}_{drift_type}_drift_{noise_value}_noise_{balance_type}{drift_width}.arff' \n",
    "\n",
    "                            print(dataset_path)\n",
    "                            # count how much it takes to run\n",
    "                            now = datetime.datetime.now()\n",
    "                            print('hour '+ str(int(now.hour)) + ' minute ' +  str(int(now.minute)) +' second ' + str(int(now.second)))\n",
    "\n",
    "\n",
    "                            # read data \n",
    "                            data = arff.loadarff(dataset_path)\n",
    "\n",
    "                            df = pd.DataFrame(data[0])\n",
    "                            df = df.replace(df['class'].unique()[0], 0)\n",
    "                            df = df.replace(df['class'].unique()[1], 1)\n",
    "\n",
    "                            # extract labels\n",
    "\n",
    "                            labels = list(df['class'])\n",
    "\n",
    "                            # one hot encoding if needed\n",
    "\n",
    "                            if(dataset_name.startswith('agraw')):\n",
    "                                one_hot_encoded_data = pd.get_dummies(df, columns = ['elevel', 'car', 'zipcode'])\n",
    "                                df = one_hot_encoded_data\n",
    "\n",
    "                            # scale if needed (only for agraw 1 & 2)\n",
    "\n",
    "                            if(dataset_name.startswith('agraw')):\n",
    "\n",
    "                                labels = df['class']\n",
    "                                scaler = MinMaxScaler(feature_range = (0,1))\n",
    "                                scaler.fit(df.drop(['class'], axis=1))\n",
    "                                df_scale = scaler.transform(df.drop(['class'], axis=1))\n",
    "                                df = pd.DataFrame(df_scale)\n",
    "                                df['class'] = labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            # split train test\n",
    "\n",
    "                            X_train, y_train = create_train(df, length_of_train)\n",
    "\n",
    "                            # PCA\n",
    "\n",
    "                            pca = PCA(n_components = 0.999)\n",
    "                            pca.fit(X_train)\n",
    "\n",
    "                            X_train_pca = pca.transform(X_train)\n",
    "\n",
    "                            # extract distributions train/pca train\n",
    "\n",
    "                            distribution_train = sns.distplot(X_train).get_lines()[0].get_data()[1]\n",
    "                            plt.close()\n",
    "\n",
    "                            distribution_train_pca = sns.distplot(X_train_pca).get_lines()[0].get_data()[1]\n",
    "                            plt.close()\n",
    "\n",
    "                            # bootstrapping\n",
    "\n",
    "\n",
    "\n",
    "                            distributions_bootstrapping = bootstrapping(X_train, bootstrapping_samples=50)\n",
    "\n",
    "                            distributions_bootstrapping_pca = bootstrapping(X_train, bootstrapping_samples=50, pca=pca)\n",
    "\n",
    "                            # extract distributions testing\n",
    "\n",
    "                            distributions_test = []\n",
    "                            distributions_test_pca = []\n",
    "\n",
    "                            start_from_sample = length_of_train\n",
    "                            for i in tqdm(range(0, int((len(df)-length_of_train)/length_of_test))):\n",
    "                                X_test, y_test = create_test(df, start_from_sample, length_of_test)\n",
    "                                start_from_sample = start_from_sample + length_of_test\n",
    "\n",
    "                                dist_test = sns.distplot(X_test).get_lines()[0].get_data()[1]\n",
    "                                plt.close()\n",
    "\n",
    "                                X_test_pca = pca.transform(X_test)\n",
    "                                dist_test_pca = sns.distplot(X_test_pca).get_lines()[0].get_data()[1]\n",
    "                                plt.close()\n",
    "\n",
    "                                distributions_test.append(dist_test)\n",
    "                                distributions_test_pca.append(dist_test_pca)\n",
    "\n",
    "                                #print(len(distributions_test))\n",
    "                                #print(len(distributions_test_pca))\n",
    "\n",
    "\n",
    "\n",
    "                            # EDE\n",
    "\n",
    "                            detected_drifts_ks = []\n",
    "                            detected_drifts_mw = []\n",
    "                            for i in range(0, len(distributions_test)):\n",
    "                                if(ede_drift_detector(distribution_train, distributions_test[i], 'ks')==1):\n",
    "                                    detected_drifts_ks.append(i)\n",
    "                                if(ede_drift_detector(distribution_train, distributions_test[i], 'mw')==1):\n",
    "                                    detected_drifts_mw.append(i)\n",
    "\n",
    "                            test = ['ks', 'mw']\n",
    "                            drifts = [detected_drifts_ks, detected_drifts_mw]\n",
    "                            df_ede_bootstrapping = pd.DataFrame({'dataset': f'{dataset_name}',\n",
    "                                                                 'drift_type': f'{drift_type}',\n",
    "                                                                 'drift_width': f'{drift_width_column}',\n",
    "                                                                 'detector' : 'ede',\n",
    "                                                                 'distance/test': test, \n",
    "                                                                 'drift_batches': drifts,\n",
    "                                                                 'noise_value': noise_value,\n",
    "                                                                 'balance_type': balance_type})\n",
    "                                                        \n",
    "\n",
    "                            # kdqTrees\n",
    "\n",
    "                            kl_drift = []\n",
    "                            mh_drift = []\n",
    "                            cbs_drift = []\n",
    "                            ksnk_drift = []\n",
    "                            csn_drift = []\n",
    "                            sqe_drift = []\n",
    "                            bct_drift = []\n",
    "\n",
    "                            for i in tqdm(range(0, len(distributions_test))):\n",
    "                                if(kdqtrees_bootstrapping(distribution_train, distributions_test[i], distributions_bootstrapping, 'kl_divergence') == 1):\n",
    "                                    kl_drift.append(i)\n",
    "                                if(kdqtrees_bootstrapping(distribution_train, distributions_test[i], distributions_bootstrapping, 'manhattan') == 1):\n",
    "                                    mh_drift.append(i)\n",
    "                                if(kdqtrees_bootstrapping(distribution_train, distributions_test[i], distributions_bootstrapping, 'chebyshev') == 1):\n",
    "                                    cbs_drift.append(i)\n",
    "                                if(kdqtrees_bootstrapping(distribution_train, distributions_test[i], distributions_bootstrapping, 'kulsinski') == 1):\n",
    "                                    ksnk_drift.append(i)\n",
    "                                if(kdqtrees_bootstrapping(distribution_train, distributions_test[i], distributions_bootstrapping, 'cosine') == 1):\n",
    "                                    csn_drift.append(i)\n",
    "                                if(kdqtrees_bootstrapping(distribution_train, distributions_test[i], distributions_bootstrapping, 'squared_euclidean') == 1):\n",
    "                                    sqe_drift.append(i)\n",
    "                                if(kdqtrees_bootstrapping(distribution_train, distributions_test[i], distributions_bootstrapping, 'bhattacharyya') == 1):\n",
    "                                    bct_drift.append(i)\n",
    "                                #print(i)\n",
    "\n",
    "                            distances = ['kl_div', 'manhattan', 'chebysev', 'kulsinski', 'cosine', 'sq_euclid', 'batthacrya']\n",
    "                            drifts = [kl_drift, mh_drift, cbs_drift, ksnk_drift, csn_drift, sqe_drift, bct_drift]\n",
    "                            df_kdqtrees_bootstrapping = pd.DataFrame({'dataset': f'{dataset_name}',\n",
    "                                                                      'drift_type': f'{drift_type}',\n",
    "                                                                      'drift_width': f'{drift_width_column}',\n",
    "                                                                      'detector' : 'kdqTrees',\n",
    "                                                                      'distance/test': distances, \n",
    "                                                                      'drift_batches': drifts,\n",
    "                                                                      'noise_value': noise_value,\n",
    "                                                                      'balance_type': balance_type})\n",
    "\n",
    "                            # PCA-kdqTrees\n",
    "\n",
    "                            kl_drift = []\n",
    "                            mh_drift = []\n",
    "                            cbs_drift = []\n",
    "                            ksnk_drift = []\n",
    "                            csn_drift = []\n",
    "                            sqe_drift = []\n",
    "                            bct_drift = []\n",
    "\n",
    "                            for i in tqdm(range(0, len(distributions_test_pca))):\n",
    "\n",
    "                                if(kdqtrees_bootstrapping(distribution_train_pca, distributions_test_pca[i], distributions_bootstrapping_pca, 'kl_divergence') == 1):\n",
    "                                    kl_drift.append(i)\n",
    "                                if(kdqtrees_bootstrapping(distribution_train_pca, distributions_test_pca[i], distributions_bootstrapping_pca, 'manhattan') == 1):\n",
    "                                    mh_drift.append(i)\n",
    "                                if(kdqtrees_bootstrapping(distribution_train_pca, distributions_test_pca[i], distributions_bootstrapping_pca, 'chebyshev') == 1):\n",
    "                                    cbs_drift.append(i)\n",
    "                                if(kdqtrees_bootstrapping(distribution_train_pca, distributions_test_pca[i], distributions_bootstrapping_pca, 'kulsinski') == 1):\n",
    "                                    ksnk_drift.append(i)\n",
    "                                if(kdqtrees_bootstrapping(distribution_train_pca, distributions_test_pca[i], distributions_bootstrapping_pca, 'cosine') == 1):\n",
    "                                    csn_drift.append(i)\n",
    "                                if(kdqtrees_bootstrapping(distribution_train_pca, distributions_test_pca[i], distributions_bootstrapping_pca, 'squared_euclidean') == 1):\n",
    "                                    sqe_drift.append(i)\n",
    "                                if(kdqtrees_bootstrapping(distribution_train_pca, distributions_test_pca[i], distributions_bootstrapping_pca, 'bhattacharyya') == 1):\n",
    "                                    bct_drift.append(i)\n",
    "                                #print(i)\n",
    "\n",
    "                            distances = ['kl_div', 'manhattan', 'chebysev', 'kulsinski', 'cosine', 'sq_euclid', 'batthacrya']\n",
    "                            drifts = [kl_drift, mh_drift, cbs_drift, ksnk_drift, csn_drift, sqe_drift, bct_drift]\n",
    "                            df_pca_bootstrapping = pd.DataFrame({'dataset': f'{dataset_name}',\n",
    "                                                                    'drift_type': f'{drift_type}',\n",
    "                                                                    'drift_width': f'{drift_width_column}',\n",
    "                                                                    'detector' : 'pca',\n",
    "                                                                    'distance/test': distances, \n",
    "                                                                    'drift_batches': drifts,\n",
    "                                                                    'noise_value': noise_value,\n",
    "                                                                    'balance_type': balance_type})\n",
    "\n",
    "                            merged_results = pd.concat([df_ede_bootstrapping, df_kdqtrees_bootstrapping, df_pca_bootstrapping])\n",
    "                            merged_results = merged_results.reset_index(drop=True)\n",
    "\n",
    "                            # Evaluation metrics\n",
    "\n",
    "                            abrupt_drift_start = 2\n",
    "                            gradual_drift_start = 2\n",
    "\n",
    "                            false_positive_rate = []\n",
    "                            latency = []\n",
    "\n",
    "                            for i in range(0, len(merged_results.drift_batches)):\n",
    "\n",
    "                                false_positive_rate.append(compute_metric_false_positive(merged_results.drift_batches[i]))\n",
    "                                latency.append(compute_metric_latency(merged_results.drift_batches[i]))\n",
    "\n",
    "                            merged_results['false_positive_rate'] = false_positive_rate\n",
    "                            merged_results['latency'] = latency\n",
    "                            merged_results['random_seed'] = [random_seed for i in range(len(merged_results.dataset))]\n",
    "\n",
    "                            final_results_df = pd.concat([final_results_df, merged_results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189ede8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df.to_csv('./DDB_drift_detection_synthetic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734d1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeebf7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9e14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e9ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d598c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e07745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf67fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b31b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
